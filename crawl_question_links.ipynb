{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "551bec27-c0e3-4bf2-a23c-261f172c19d4",
   "metadata": {},
   "source": [
    "# Crawl tailieumoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "aa8a9fe1-97a2-466d-ad4d-55f8d6f942f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "188fc7be-f62f-4966-9864-42eb4f53b6d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "id_to_subject = {\n",
    "    1: \"toan\",\n",
    "    2: \"van\",\n",
    "    3: \"vatly\",\n",
    "    4: \"hoa\",\n",
    "    6: \"lichsu\",\n",
    "    7: \"dialy\",\n",
    "    8: \"sinhhoc\",\n",
    "    9: \"congnghe\",\n",
    "    10: \"gdcd\",\n",
    "    14: \"tinhoc\",\n",
    "    32: \"gdqp\",\n",
    "    34: \"ktpl\",\n",
    "    18: \"lichsudialy\",\n",
    "    27: \"khoahoctunhien\"\n",
    "}\n",
    "\n",
    "# _class = 9\n",
    "# subject_id = 3\n",
    "# num_pages = 148\n",
    "# URL_prefix = f\"https://tailieumoi.vn/danh-sach-cau-hoi?lop={_class}&mon={subject_id}&page=\"\n",
    "#     def get_question_links(page: int):\n",
    "#         links = []\n",
    "#         page = requests.get(URL_prefix + str(page))\n",
    "#         soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "#         results = soup.find_all(\"a\", {\"class\": \"cl3 fs-18\"}, href=True)\n",
    "#         for result in results:\n",
    "#             links.append(result[\"href\"])\n",
    "#         return links\n",
    "\n",
    "def get_question_links(_input):\n",
    "    _class: int = _input[\"class\"]\n",
    "    _subject_id: int = _input[\"subject_id\"]\n",
    "    _page: int = _input[\"page\"]\n",
    "    URL_prefix = f\"https://tailieumoi.vn/danh-sach-cau-hoi?lop={_class}&mon={_subject_id}&page=\"\n",
    "    links = []\n",
    "    webpage = requests.get(URL_prefix + str(_page))\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    results = soup.find_all(\"a\", {\"class\": \"cl3 fs-18\"}, href=True)\n",
    "    for result in results:\n",
    "        links.append(result[\"href\"])\n",
    "    return links\n",
    "        \n",
    "\n",
    "num_procs = 10\n",
    "def crawl_question_links(_class: int, subject_id: int, num_pages: int):        \n",
    "    with Pool(num_procs) as p:\n",
    "        results = list(tqdm(\n",
    "            p.imap(\n",
    "                get_question_links, \n",
    "                [{\"class\": _class, \"subject_id\": subject_id, \"page\": page} for page in range(1, num_pages+1)]\n",
    "            ), \n",
    "            total=len(range(1, num_pages+1))\n",
    "        ))\n",
    "    results1 = [link for links in results for link in links]\n",
    "    if not results1:\n",
    "        return\n",
    "    with open(f\"./data/question_links/{id_to_subject[subject_id]}{_class}_links.txt\", \"w\") as fp:\n",
    "        for link in results1:\n",
    "            # write each item on a new line\n",
    "            fp.write(\"%s\\n\" % link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7521f960-bcdc-4991-b48f-663e4b27f4f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "00%|██████████| 500/500 [00:48<00:00, 10.27it/s]"
     ]
    }
   ],
   "source": [
    "for _class in range(6, 10):\n",
    "    for subject_id in id_to_subject.keys():\n",
    "        crawl_question_links(_class, subject_id, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a1816df4-1d9a-4c79-a8d9-27ea6441290b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# URL_prefix = \"https://tailieumoi.vn/danh-sach-cau-hoi?lop=12&mon=1&page=2\"\n",
    "# links = []\n",
    "# for i in tqdm(range(1, 1025)):\n",
    "#     page = requests.get(URL_prefix + str(i))\n",
    "#     soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "#     results = [item.a for item in soup.find_all(\"div\", {\"class\": \"cau-hoi-list-item-title overflow-x-el\"})]\n",
    "#     for result in results:\n",
    "#         links.append(result[\"href\"])\n",
    "        \n",
    "# open file in write mode\n",
    "with open(f\"./data/question_links/{id_to_subject[subject_id]}{_class}_links.txt\", \"a\") as fp:\n",
    "    for link in results1:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225782c1-90d9-4c2b-a5a5-9afd17468c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e3a59a6-f490-40b9-ad90-bcada4c760a0",
   "metadata": {},
   "source": [
    "# Crawl vietjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a18900-3f7a-4c16-913e-04b23dcccf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7db493f-2ea2-466c-9d8d-65e9f393f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def get_question_links(_page: int):\n",
    "    URL_prefix = f\"https://khoahoc.vietjack.com/cau-hoi/mon-dai-hoc/trac-nghiem-tong-hop?page=\"\n",
    "    links = []\n",
    "    webpage = requests.get(URL_prefix + str(_page))\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    results = [item.a for item in soup.find_all(\"div\", {\"class\": \"cau-hoi-list-item-title overflow-x-el\"})]\n",
    "    for result in results:\n",
    "        links.append(result[\"href\"])\n",
    "    return links\n",
    "        \n",
    "\n",
    "num_procs = 10\n",
    "def crawl_question_links(num_pages: int):        \n",
    "    with Pool(num_procs) as p:\n",
    "        results = list(tqdm(\n",
    "            p.imap(\n",
    "                get_question_links, \n",
    "                range(1, num_pages+1)\n",
    "            ), \n",
    "            total=len(range(1, num_pages+1))\n",
    "        ))\n",
    "    results1 = [link for links in results for link in links]\n",
    "    if not results1:\n",
    "        return\n",
    "    with open(f\"./data/question_links/vietjack/daihoc_links.txt\", \"w\") as fp:\n",
    "        for link in results1:\n",
    "            # write each item on a new line\n",
    "            fp.write(\"%s\\n\" % link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0277b7e0-1c3a-4162-b942-e8fc5282792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1703/1703 [14:56<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "crawl_question_links(1703)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7b86a9d-90dc-4ca7-bf3c-b5dbbdfce428",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/question_links/vietjack/daihoc_links.txt\", \"r\") as f:\n",
    "    urls = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d6d44ee-ac31-4353-a6cc-435e45f9cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    start = i*20000\n",
    "    if i>len(urls)-20000:\n",
    "        end = len(urls)\n",
    "    else:\n",
    "        end = start+20000\n",
    "    with open(f\"./data/question_links/vietjack/daihoc_{i}_links.txt\", \"w\") as fp:\n",
    "        for link in urls[start:end]:\n",
    "            # write each item on a new line\n",
    "            fp.write(\"%s\\n\" % link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b467e68-8f39-4791-a8b8-338c7cf3d243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85150"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c888cb58-17a2-4386-a3ac-50fae8fba539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d39e97-0ca6-414f-8d76-180f7c9e956f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/question_links/vietjack/daihoc_links.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./data/question_links/vietjack/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ad997-263a-4474-9003-d3311ced435f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
